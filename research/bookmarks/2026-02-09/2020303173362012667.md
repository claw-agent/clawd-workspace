# Bookmark Analysis: 2020303173362012667

## Basic Info
- **Author:** Ethan Mollick (@emollick)
- **URL:** https://x.com/emollick/status/2020303173362012667
- **Date:** Sun Feb 08 01:06:26 +0000 2026
- **Engagement:** 1849 â¤ï¸ | 207 ðŸ” | 156 ðŸ’¬
- **Content Type:** Tweet (Essay-length thread)

## The Tweet
> I think agentic AI would work much better if people took lessons from organizational theory, which has actually spent a lot of time understanding how to deal with complex hierarchies, information limits, and spans of control.
>
> Right now most agentic AI systems seem to pretend that models have basically unlimited ability to manage subagents when that is clearly not true. We need measures of spans of control for AI. A human tops out at less than 10 direct reports. I am pretty sure that 100 subagents is too much for an orchestrator agent - suspect we need middle management agents (yes, I get it, insert middle management joke here).
>
> Similarly, we need more attention to boundary objects. These are what is handed between groups (marketing to IT to sales) in organizations to convey meaning as a project crosses group boundaries, like a prototype or a user story. Right now agents pass raw text & maybe code back and forth. Structured boundary objects that multiple agents of different ability levels can read and write to would solve a huge number of coordination failures & reduce token use.
>
> I also think about coupling, which is how tightly units inside organizations are bound. Most agentic systems are either too tightly coupled (every step needs approval) or too loose (Moltbook). This tradeoff is well-studied in organizations, I bet a lot would apply to agents. Other known issues like bounded rationality also apply, I suspect.
>
> Everyone is rushing towards the (terribly named) agent swarm, but the issue won't just be how good the model is, it will be org design choices. I am not sure the labs see this, but we definitely need a lot more experiments with organizing agents done by people who understand real coordination issues.

## Summary
Ethan Mollick argues that agentic AI systems should borrow from organizational theory â€” concepts like spans of control, boundary objects, coupling, and bounded rationality are directly applicable to multi-agent coordination.

## Deep Analysis
This is a high-signal post that maps organizational science concepts to multi-agent AI:

1. **Spans of Control:** Humans max at ~10 direct reports. Orchestrator agents likely fail well before 100 subagents. Implies need for "middle management" agent layers.

2. **Boundary Objects:** Structured artifacts passed between agent groups (not just raw text). Think: schemas, prototypes, user stories as formal handoff formats. Would reduce coordination failures AND token usage.

3. **Coupling:** Too tight (approval on every step) vs too loose (fire-and-forget swarms). The sweet spot is well-studied in org theory.

4. **Bounded Rationality:** Agents, like humans, have limited info processing. System design must account for this.

This directly validates our AGENTS.md multi-agent coordination rules (CooperBench findings about 50% worse outcomes from uncoordinated agents, orchestrator pattern, sequential pipelines).

## Why This Matters
**Extremely relevant.** We already implement several of these principles:
- Our CooperBench rules limit parallel agents on shared state
- We use orchestrator pattern (one coordinator â†’ specialists)
- We do sequential pipelines with clean handoffs

But we're missing:
- Formal "boundary objects" between agents (structured schemas for handoffs)
- Explicit span of control limits (we don't cap subagent count)
- Middle management layer for complex multi-agent work

## Action Items
- [ ] Consider adding explicit span-of-control limit to AGENTS.md (max 5-7 subagents per orchestrator)
- [ ] Design structured handoff schemas ("boundary objects") for common agent-to-agent workflows
- [ ] Research organizational theory concepts for more agent coordination patterns
